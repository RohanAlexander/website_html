<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Course that allows students a chance to build things with AI.">
    <title>Courses: Applications of Natural Language Processing</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital@0;1&display=swap" rel="stylesheet">
    <style>
      *, *::before, *::after {
        box-sizing: border-box;
      }
      body {
        font-family: "EB Garamond", serif;
        margin: 0;
        padding: 20px;
        background: #fff;
        color: #000;
        line-height: 1.3;
      }
      .container {
        max-width: 210mm; /* A4 width */
        margin: 0 auto;
        padding: 0 10px; 
      }
      header,
      nav,
      section {
        margin-bottom: 20px;
      }
      a {
        color: #000;
        text-decoration: none;
        transition: color 0.3s ease, text-decoration 0.3s ease;
      }
      a:hover {
        color: #555;
        text-decoration: underline;
      }
      nav a {
        margin-right: 15px;
        text-decoration: underline;
      }
    </style>
  </head>
<body>
  <div class="container">
    <header>
      <h1>Courses</h1>
        <nav>
          <a href="../index.html">Home</a>
          <a href="../pdfs/cv-academic-Rohan_Alexander.pdf" target="_blank">CV</a>
          <a href="../bookshelf.html">Bookshelf</a>
          <a href="../tdw.html">Workshop</a>
          <a href="../misc.html">Misc</a>
          <a href="../blog.html">Blog</a>
        </nav>
    </header>
    <section>

      <h2>Applications of Natural Language Processing</h2>

        <p>Last updated: 2025-02-25</p>

        <p><i>This course may be taken as a reading course in either DoSS of Information. Please get in touch if interested.</i></p>

        <h3>Overview</h3>
          <p>
            The purpose of this course is to develop students who can:
          </p>

          <ul>
            <li>work productively to implement existing NLP methods, especially LLMs; and</li>
            <li>use NLP, and especially LLMs, to contribute to our understanding of the world; and</li>
            <li>engage in thoughtful, ethical, critique of NLP, especially LLMs.</li>
          </ul>

        <h3>Learning objectives</h3>
          <p>By the end of the course, you should have:</p>

          <ol>
              <li>an understanding of NLP, especially LLMs, and its place in the world;</li>
              <li>exceptional written and verbal communication skills; and </li>
              <li>contribute in some small way to our understanding of something related to NLP, ideally LLMs.</li>
          </ol>

        <h3>Pre-requisites</h3>

          <ul>
            <li>Comfortable coding in R and Python and using GitHub.</li>
            <li>Go through Silge, Julia, & David Robinson, 2020, <i>Text Mining with R</i>, https://www.tidytextmining.com.</li>
          </ul>

        <h3>Content</h3>

        <h4>Week 1 "Overview"</h4>
        <ul>
          <li>Karpathy, Andrej, 2025, "<a href="https://youtu.be/7xTGNNLPyMI" target="_blank">Intro to Large Language Models</a>", <i>YouTube</i>, 5 February.</li>
        </ul>

        <h4>Week 2 "Tokenisation and n-grams"</h4>
        <ul>
          <li>Hvitfeldt, Emil & Julia Silge, 2020, <i>Supervised Machine Learning for Text Analysis in R</i>, <a href="https://smltar.com/tokenization" target="_blank">Chapter 2 Tokenization</a>.</li>
          <li>Jurafsky, Dan, and James H. Martin, 2024, <i>Speech and Language Processing</i>, 3rd ed., <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Chapter 3 N-gram Language Models</a>.</li>
        </ul>

        <h4>Week 3 "Embeddings"</h4>
        <ul>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, Chapter 2 Tokens and Embeddings</li>
          <li>Boykis, Vicky, 2023, "What are embeddings", <a href="https://vickiboykis.com/what_are_embeddings/" target="_blank">https://vickiboykis.com/what_are_embeddings/</a></li>
        </ul>

        <h4>Week 4 "Modeling"</h4>

        <ul>
          <li>Hvitfeldt, Emil & Julia Silge, 2020, <i>Supervised Machine Learning for Text Analysis in R</i>, Chapters 4-6, <a href="https://smltar.com" target="_blank">https://smltar.com</a></li>
          <li>Jurafsky, Dan, and James H. Martin, 2024, <i>Speech and Language Processing</i>, 3rd ed., Chapters 4 and 5, <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">https://web.stanford.edu/~jurafsky/slp3/</a></li>
        </ul>

        <h4>Week 5 "Neural Nets I"</h4>

        <ul>
          <li>Jurafsky, Dan, and James H. Martin, 2020, <i>Speech and Language Processing</i>, 3rd ed., Chapters 6 and 7, <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">https://web.stanford.edu/~jurafsky/slp3/</a></li>
          <li>François Chollet, 2021, <i>Deep Learning with Python</i>, Chapters 1-4.</li>
        </ul>

        <h4>Week 6 "Neural Nets II"</h4>

        <ul>
          <li>Hvitfeldt, Emil & Julia Silge, 2020, <i>Supervised Machine Learning for Text Analysis in R</i>, Chapters 7-9, <a href="https://smltar.com" target="_blank">https://smltar.com</a></li>
        </ul>

        <h4>Week 7 "Deep learning I"</h4>

        <ul>
          <li>François Chollet, 2021, <i>Deep Learning with Python</i>, Chapter 6</li>
        </ul>

        <h4>Week 8 "LSTM"</h4>

        <ul>
          <li>Jurafsky, Dan, and James H. Martin, 2020, <i>Speech and Language Processing</i>, 3rd ed., Chapters 8 and 9, <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">https://web.stanford.edu/~jurafsky/slp3/</a></li>
        </ul>

        <h4>Week 9 "Transformers I"</h4>

        <ul>
          <li>Karpathy, Andrej, 2022, "Neural Networks: Zero to Hero", Both of 'The spelled-out intro...' videos.</li>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, O'Reilly, Chapters 1-3.</li>
        </ul>

        <h4>Week 10 "Transformers II"</h4>

        <ul>
          <li>Manning, Vaswani and Huang, 2019, 'Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention', <a href="https://www.youtube.com/watch?v=5vcj8kSwBCY" target="_blank">https://www.youtube.com/watch?v=5vcj8kSwBCY</a></li>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, O'Reilly, Chapters 4-5.</li>
          <li>Uszkoreit, Jakob, 2017, 'Transformer: A Novel Neural Network Architecture for Language Understanding', <i>Google AI Blog</i>, 31 August, <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></li>
        </ul>

        <h4>Week 11 "Transformers III"</h4>

        <ul>
          <li>Karpathy, Andrej, 2022, "Neural Networks: Zero to Hero", The 'Building make more…' videos</li>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, O'Reilly, Chapters 6-8.</li>
          <li>Ashish Vaswani, et al., 2017, 'Attention Is All You Need', <i>arXiv</i>, <a href="http://arxiv.org/abs/1706.03762" target="_blank">http://arxiv.org/abs/1706.03762</a></li>
          <li>Jacob Devlin and Ming-Wei Chang, 2018, 'Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing', <i>Google AI Blog</i>, <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" target="_blank">https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html</a></li>
          <li>Jacob Devlin, et al., 2018, 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', <i>arXiv</i>, <a href="https://arxiv.org/abs/1810.04805" target="_blank">https://arxiv.org/abs/1810.04805</a></li>
          <li>Rush, Alexander, 2018, 'The Annotated Transformer', <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
        </ul>

        <h4>Week 12 "Transformers IV"</h4>

        <ul>
          <li>Karpathy, Andrej, 2022, "Neural Networks: Zero to Hero", The 'Let's build GPT: from scratch, in code, spelled out.'</li>
          <li>Tom B. Brown, et al., 2020, 'Language Models are Few-Shot Learners', <i>arXiv</i>, <a href="https://arxiv.org/abs/2005.14165" target="_blank">https://arxiv.org/abs/2005.14165</a></li>
        </ul>

        <h3>Assessment</h3>

        <h4>Notebook</h4>

        <ul>
          <li>Due date: Try to keep this updated weekly, on average, over the course of the term.</li>
          <li>Task: Use Quarto to keep a notebook of what you read in the style of <a href="https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html" target="_blank">this one</a> by Andrew Heiss.</li>
          <li>Weight: 25 per cent.</li>
        </ul>

        <h4>Paper #1</h4>

        <ul>
          <li>Due date: Thursday, noon, Week 1.</li>
          <li>Task: <a href="https://tellingstorieswithdata.com/23-assessment.html#sec-paper-one" target="_blank">Donaldson Paper</a> (although vary the dataset, so that it is something related to NLP).</li>
          <li>Weight: 10 per cent.</li>
        </ul>

        <h4>Paper #2</h4>

        <ul>
          <li>Due date: Thursday, noon, Week 6.</li>
          <li>Task: Write a paper applying what you are learning.</li>
          <li>Weight: 25 per cent.</li>
        </ul>

        <h4>Final Paper</h4>

        <ul>
          <li>Due date: Thursday, noon, Week 12 + two weeks.</li>
          <li>Task: Write a paper that involves doing original research.</li>
          <li>Weight: 40 per cent.</li>
        </ul>

      </section>
    </div>
  </body>
</html>