<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Course that allows students a chance to build things with AI.">
    <title>Courses: Applications of Natural Language Processing</title>
    <link rel="stylesheet" href="../styles.css">
  </head>
<body>
  <div class="container">
    <header>
      <h1>Courses</h1>
        <nav>
          <a href="../index.html">Home</a>
          <a href="../pdfs/cv-academic-Rohan_Alexander.pdf" target="_blank">CV</a>
          <a href="../bookshelf.html">Bookshelf</a>
          <a href="../tdw.html">Workshop</a>
          <a href="../misc.html">Misc</a>
          <a href="../blog.html">Blog</a>
        </nav>
    </header>
    <section>

      <h2>Applications of Natural Language Processing</h2>

        <p>Last updated: 2025-03-06</p>

        <p><i>This course may be taken as a reading course in DoSS or Information. Please get in touch if interested.</i></p>

        <h3>Overview</h3>
          <p>
            The purpose of this course is to develop students who can use NLP methods, and especially LLMs, to:
          </p>

          <ul>
            <li>work productively to implement existing methods;</li>
            <li>contribute to our understanding of the world; and</li>
            <li>engage in thoughtful, ethical, critique.</li>
          </ul>

        <h3>Learning objectives</h3>
          <p>By the end of the course, you should have:</p>

          <ol>
              <li>an good understanding of applied NLP, especially LLMs, and its place in the world;</li>
              <li>exceptional written and verbal communication skills; and </li>
              <li>contributed in some small way to our understanding of some aspect of the world.</li>
          </ol>

        <h3>Pre-requisites</h3>

          <ul>
            <li>Comfortable with R and Python, and using GitHub.</li>
            <li>Go through Silge, Julia, & David Robinson, 2020, <a href="https://www.tidytextmining.com" target="_blank"><i>Text Mining with R</i></a>.</li>
          </ul>

        <h3>Content</h3>

        <h4>Week 1 "Overview"</h4>
        <ul>
          <li>Karpathy, Andrej, 2025, "<a href="https://youtu.be/7xTGNNLPyMI" target="_blank">Intro to Large Language Models</a>", <i>YouTube</i>, 5 February.</li>
          <li>Augenstein, Isabelle, 2025, "<a href="https://isabelleaugenstein.github.io/slides/2025_KHIPU_IntroNLP.pdf" target="_blank">Slides: Natural Language Processing
Fundamentals</a>", 10 March.</li>
        </ul>

        <h4>Week 2 "Tokenisation and n-grams"</h4>
        <ul>
          <li>Hvitfeldt, Emil & Julia Silge, 2020, <i>Supervised Machine Learning for Text Analysis in R</i>, <a href="https://smltar.com/tokenization" target="_blank"">Chapter 2 Tokenization</a>.</li>
          <li>Jurafsky, Dan, and James H. Martin, 2024, <i>Speech and Language Processing</i>, 3rd ed., <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank"">Chapter 3 N-gram Language Models</a>.</li>
        </ul>

        <h4>Week 3 "Embeddings"</h4>
        <ul>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, Chapter 2 Tokens and Embeddings</li>
          <li>Boykis, Vicky, 2023, <i><a href="https://vickiboykis.com/what_are_embeddings/" target="_blank">What are embeddings</a></i>, 10.5281/zenodo.8015029.</li>
        </ul>

        <h4>Week 4 "Modeling"</h4>
        <ul>
          <li>Hvitfeldt, Emil & Julia Silge, 2020, <i>Supervised Machine Learning for Text Analysis in R</i>, <a href="https://smltar.com" target="_blank"">Chapters 4-6</a>.</li>
          <li>Jurafsky, Dan, and James H. Martin, 2024, <i>Speech and Language Processing</i>, 3rd ed., <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank"">Chapters 4 and 5</a>.</li>
        </ul>

        <h4>Week 5 "Neural Nets I"</h4>
        <ul>
          <li>Jurafsky, Dan, and James H. Martin, 2020, <i>Speech and Language Processing</i>, 3rd ed., <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank"">Chapters 6 and 7</a>.</li>
          <li>François Chollet, 2021, <i>Deep Learning with Python</i>, Chapters 1-4.</li>
        </ul>

        <h4>Week 6 "Neural Nets II"</h4>

        <ul>
          <li>Hvitfeldt, Emil & Julia Silge, 2020, <i>Supervised Machine Learning for Text Analysis in R</i>, <a href="https://smltar.com" target="_blank"">Chapters 7-9</a>.</li>
        </ul>

        <h4>Week 7 "Deep learning I"</h4>

        <ul>
          <li>François Chollet, 2021, <i>Deep Learning with Python</i>, Chapter 6.</li>
        </ul>

        <h4>Week 8 "LSTM"</h4>

        <ul>
          <li>Jurafsky, Dan, and James H. Martin, 2020, <i>Speech and Language Processing</i>, 3rd ed., <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank"">Chapters 8 and 9</a>.</li>
        </ul>

        <h4>Week 9 "Transformers I"</h4>

        <ul>
          <li>Karpathy, Andrej, 2022, "<a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank"">Neural Networks: Zero to Hero - The spelled-out intro videos</a>".</li>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, O'Reilly, Chapters 1-3.</li>
        </ul>

        <h4>Week 10 "Transformers II"</h4>

        <ul>
          <li>Manning, Vaswani and Huang, 2019, "<a href="https://www.youtube.com/watch?v=5vcj8kSwBCY" target="_blank"">Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention</a>".</li>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, O'Reilly, Chapters 4-5.</li>
          <li>Uszkoreit, Jakob, 2017, "<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank"">Transformer: A Novel Neural Network Architecture for Language Understanding</a>", <i>Google AI Blog</i>, 31 August.</li>
        </ul>

        <h4>Week 11 "Transformers III"</h4>

        <ul>
          <li>Karpathy, Andrej, 2022, "<a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank"">Neural Networks: Zero to Hero - Building make more videos</a>".</li>
          <li>Alammar, Jay, and Maarten Grootendorst, 2024, <i>Hands-On Large Language Models</i>, O'Reilly, Chapters 6-8.</li>
          <li>Ashish Vaswani, et al., 2017, "<a href="http://arxiv.org/abs/1706.03762" target="_blank"">Attention Is All You Need</a>", <i>arXiv</i>.</li>
          <li>Jacob Devlin and Ming-Wei Chang, 2018, "<a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" target="_blank"">Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing</a>", <i>Google AI Blog</i>.</li>
          <li>Jacob Devlin, et al., 2018, "<a href="https://arxiv.org/abs/1810.04805" target="_blank"">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>", <i>arXiv</i>.</li>
          <li>Rush, Alexander, 2018, "<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank"">The Annotated Transformer</a>".</li>
        </ul>

        <h4>Week 12 "Transformers IV"</h4>

        <ul>
          <li>Karpathy, Andrej, 2022, "<a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank"">Let's build GPT: from scratch, in code, spelled out</a>".</li>
          <li>Tom B. Brown, et al., 2020, "<a href="https://arxiv.org/abs/2005.14165" target="_blank"">Language Models are Few-Shot Learners</a>", <i>arXiv</i>.</li>
        </ul>

        <h3>Assessment</h3>

        <h4>Notebook</h4>

        <ul>
          <li>Due date: Try to keep this updated weekly, on average, over the course of the term.</li>
          <li>Task: Use Quarto to keep a notebook of what you read in the style of <a href="https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html" target="_blank">this one</a> by Andrew Heiss.</li>
          <li>Weight: 25 per cent.</li>
        </ul>

        <h4>Paper #1</h4>

        <ul>
          <li>Due date: Thursday, noon, Week 1.</li>
          <li>Task: <a href="https://tellingstorieswithdata.com/23-assessment.html#sec-paper-one" target="_blank">Donaldson Paper</a> (although vary the dataset, so that it is something related to NLP).</li>
          <li>Weight: 10 per cent.</li>
        </ul>

        <h4>Paper #2</h4>

        <ul>
          <li>Due date: Thursday, noon, Week 6.</li>
          <li>Task: Write a paper applying what you are learning.</li>
          <li>Weight: 25 per cent.</li>
        </ul>

        <h4>Final Paper</h4>

        <ul>
          <li>Due date: Thursday, noon, Week 12 + two weeks.</li>
          <li>Task: Write a paper that involves doing original research.</li>
          <li>Weight: 40 per cent.</li>
        </ul>

      </section>
    </div>
  </body>
</html>