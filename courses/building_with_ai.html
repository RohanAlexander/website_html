<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Course that allows students a chance to build things with AI.">
  <title>Courses: Building with AI</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital@0;1&display=swap" rel="stylesheet">
  <style>
    *, *::before, *::after {
      box-sizing: border-box;
    }
    body {
      font-family: "EB Garamond", serif;
      margin: 0;
      padding: 20px;
      background: #fff;
      color: #000;
      line-height: 1.3;
    }
    .container {
      max-width: 210mm; /* A4 width */
      margin: 0 auto;
      padding: 0 10px; 
    }
    header,
    nav,
    section {
      margin-bottom: 20px;
    }
    a {
      color: #000;
      text-decoration: none;
      transition: color 0.3s ease, text-decoration 0.3s ease;
    }
    a:hover {
      color: #555;
      text-decoration: underline;
    }
    nav a {
      margin-right: 15px;
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>Courses</h1>
      <nav>
        <a href="../index.html">Home</a>
        <a href="../pdfs/cv-academic-Rohan_Alexander.pdf" target="_blank">CV</a>
        <a href="../bookshelf.html">Bookshelf</a>
        <a href="../tdw.html">Workshop</a>
        <a href="../misc.html">Misc</a>
        <a href="../blog.html">Blog</a>
      </nav>
    </header>
    <section>

      <h2>Let's (sensibly) build cool (responsible) AI stuff</h2>

      <p>Last updated: 2025-02-28</p>

      <p><i>This course may be taken as a reading course in DoSS or Information. Please get in touch if interested.</i></p>

      <h3>Overview</h3>

      <p>The purpose of this course is to demonstrate the ability to sensibly build a responsible product with LLMs. Each week you will be expected to read papers and use AI-based tools to build things.</p>

      <p>This course would suit students interested in self-driven learning and exploring how to use the exciting new AI-based tools, such as Replit Agent, Windsurf/Cursor, Lovable, Bolt, Qodo, to build a product in a responsible way. You should have familiarity with a programming language, such as Python or R, confidence using GitHub, and comfort being self-sufficient and self-motivated.</p>

      <h3>Learning objectives</h3>
      <ul>
        <li>Demonstrate the ability to use AI-enhanced tools to build responsible product prototypes.</li>
        <li>Show an understanding of the current state of responsible AI.</li>
        <li>Develop and implement a monetization strategy for a product.</li>
        <li>Evaluate products using an appropriate sampling strategy and communicate findings.</li>
      </ul>
      
      <h3>Content</h3>

      <h4>Week 1: Overview</h4>
      <ul>
        <li>Build:</li>
        <ul>
          <li>https://youtu.be/7xTGNNLPyMI</li>
        </ul>
        <li>Responsibly:</li>
        <ul>
          <li>Cantwell Smith, Brian, 2019, *The Promise of AI*, MIT Press, Chapters 10-12.</li>
        </ul>
      </ul>
      <h4>Week 2: Models</h4>
      <ul>
        <li>Build:</li>
        <ul>
          <li>https://nullprogram.com/blog/2024/11/10/</li>
        </ul>
        <li>Responsibly:</li>
        <ul>
          <li>Suresh, Harini, and John V. Guttag, 2019, 'A Framework for Understanding Unintended Consequences of Machine Learning', *arXiv*, 1901.10002, https://arxiv.org/abs/1901.10002.</li>
        </ul>
      </ul>
      <h4>Week 3: Prompts</h4>
      <ul>
        <li>Build:</li>
        <ul>
          <li>Christopher Barrie, Elli Palaiologou, and Petter Törnberg. "Prompt stability scoring for text annotation with large language models," arXiv:2407.02039 (2024).</li>
        </ul>
        <li>Responsibly:</li>
        <ul>
          <li>Boykis, Vicki, 2019, 'Neural nets are just people all the way down', 16 October, https://vicki.substack.com/p/neural-nets-are-just-people-all-the.</li>
          <li>Crawford, Kate, and Vladan Joler, 2018, 'Anatomy of an AI System: The Amazon Echo As An Anatomical Map of Human Labor, Data and Planetary Resources', *AI Now Institute and Share Lab*, 7 September, https://anatomyof.ai.</li>
          <li>Crawford, Kate, 2020, 'Kate Crawford: Anatomy of AI', Lecture, University of New South Wales, 28 January, https://youtu.be/uM7gqPnmDDc.</li>
          <li>Kitchin, Rob, 2014, *The data revolution: Big data, open data, data infrastructures and their consequences*, Sage, Introduction, Chapters 8, and 10.</li>
        </ul>
      </ul>
      <h4>Week 4: Evals</h4>
      <ul>
        <li>Build:</li>
        <ul>
          <li>https://www.propel.app/insights/building-a-snap-llm-eval-part-1/</li>
        </ul>
        <li>Responsibly:</li>
        <ul>
          <li>- Boyd, Danah, and Kate Crawford, 2012, 'Critical Questions for Big Data', *Information, Communication & Society*, 15(55), 662-679, https://www.microsoft.com/en-us/research/wp-content/uploads/2012/05/CriticalQuestionsForBigDataICS.pdf.</li>
          <li>Denton, Emily, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole, Morgan Klaus Scheuerman, 2020, 'Bringing the People Back In: Contesting Benchmark Machine Learning Datasets', *arXiv*, 14 July, https://arxiv.org/abs/2007.07399. </li>
        </ul>
      </ul>
      <h4>Week 5: Data</h4>
      <ul>
        <li>Build:</li>
        <ul>
          <li>Hwang, Jackelyn, Nima Dahir, Mayuka Sarukkai, Gabby Wright. 2023. "Curating Training Data for Reliable Large-Scale Visual Data Analysis: Lessons from Identifying Trash in Street View Imagery." Sociological Methods & Research 52(3):1155–1200.</li>
        </ul>
        <li>Responsibly:</li>
        <ul>
          <li>D'Ignazio, Catherine, and Lauren F. Klein, 2020, *Data Feminism*, MIT Press.</li>
          <li>Gebru, Timnit, 2020, 'Race and Gender', *The Oxford Handbook of Ethics of AI*, Chapter 13, Oxford University Press.</li>
        </ul>
      </ul>
      <h4>Week 6: Ideas</h4>
      <ul>
        <li>Build:</li>
        <ul>
          <li>https://paulgraham.com/startupideas.html</li>
        </ul>
        <li>Responsibly:</li>
        <ul>
          <li>Davidson, Thomas, Debasmita Bhattacharya, and Ingmar Weber, 2019, 'Racial bias in hate speech and abusive language detection datasets', arXiv, https://arxiv.org/abs/1905.12516.</li>
          <li>Noble, Safiya Umoja, 2018, *Algorithms of Oppression: How Search Engines Reinforce Racism*, NYU Press, Chapter 2.</li>
        </ul>
      </ul>
      <h4>Week 7: AI and social sciences</h4>
      <ul>
        <li>Build:</li>
        <ul>
          <li>Bail, Christopher, 2024, "Can Generative AI improve social science?", <i>Proceedings of the National Academy of Sciences</i>, 121 (21), <a href="https://doi.org/10.1073/pnas.2314021121">10.1073/pnas.2314021121</a></li>
          <li>Grossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham, 2023, "AI and the Transformation of Social Science Research", <i>Science</i>, 380 (6650), <a href="https://doi.org/10.1126/science.adi1778">10.1126/science.adi1778</a> pp. 1108-1109.</li>
          <li>Mellon, Jonathan, Jack Bailey, Ralph Scott, James Breckwoldt, Marta Miori, and Phillip Schmedeman, 2024, "Do AIs know what the most important issue is? Using language models to code open-text social survey responses at scale", <i>Research & Politics</i>, 11 (1), <a href="https://doi.org/10.1177/20531680241231468">10.1177/20531680241231468</a>.</li>
        </ul>
      </ul>
      <li>Responsibly:</li>
      <ul>
        <li>- Bender, Emily M., Angelina McMillan-Major, Timnit Gebru and Shmargaret Shmitchell, 2021, 'On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?', https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf</li>
        <li>Hovy, Dirk and Shannon L. Spruit, 2016, 'The Social Impact of Natural Language Processing', Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pp. 591–598, https://aclweb.org/anthology/P16-2096.pdf.</li>
        <li>Prabhumoye, Shrimai, Elijah Mayfield, and Alan W Black, 2019, 'Principled Frameworks for Evaluating Ethics in NLP Systems', Proceedings of the 2019 Workshop on Widening NLP, https://aclweb.org/anthology/W19-3637/.</li>
      </ul>
    </ul>
    <h4>Week 8</h4>
    <ul>
      <li>Build:</li>
      <ul>
        <li></li>
      </ul>
      <li>Responsibly:</li>
      <ul>
        <li>Brundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, Dario Amodei, 2019, 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation', arXiv, https://arxiv.org/abs/1802.07228.</li>
      </ul>
    </ul>
    <h4>Week 9</h4>
    <ul>
      <li>Build:</li>
      <ul>
        <li></li>
      </ul>
      <li>Responsibly:</li>
      <ul>
        <li>Spiegelhalter, David, 2020, 'Should We Trust Algorithms?', Harvard Data Science Review, 31 January, https://doi.org/10.1162/99608f92.cb91a35a.</li>
        <li>Molnar, Petra and Lex Gill, 2018, 'Bots at the Gate: A Human Rights Analysis of Automated Decision-Making in Canada's Immigration and Refugee System,' Citizen Lab and International Human Rights Program, Faculty of Law, University of Toronto, Research Report No. 114, University of Toronto, September, https://citizenlab.ca/wp-content/uploads/2018/09/IHRP-Automated-Systems-Report-Web-V2.pdf. </li>
      </ul>
    </ul>
    <h4>Week 10</h4>
    <ul>
      <li>Build:</li>
      <ul>
        <li></li>
      </ul>
      <li>Responsibly:</li>
      <ul>
        <li></li>
      </ul>
    </ul>
    <h4>Week 11</h4>
    <ul>
      <li>Build:</li>
      <ul>
        <li></li>
      </ul>
      <li>Responsibly:</li>
      <ul>
        <li></li>
      </ul>
    </ul>
    <h4>Week 12</h4>
    <ul>
      <li>Build:</li>
      <ul>
        <li></li>
      </ul>
      <li>Responsibly:</li>
      <ul>
        <li></li>
      </ul>
    </ul>

    <h3>Assessment</h3>

    <p>Assessment ensures that you have internalized the sensible/responsible aspects of the course and the build : There will be two written exams during the semester. 

    There will be two mini-projects due during the semester.  The final assessment will be a third product that you will be expected to have actual paying users for. You will come up with the idea underpinning this yourself.</p>
    
    <h4>In-class papers</h4>

    <ul>
      <li>Due dates: Weeks 3, 6, and 9.</li>
      <li>Weight: Each is worth 20 per cent; the worst one is dropped.</li>
      <li>Task: Write a paper, in exam conditions, of 2-6 pages on a topic covered in the preceding weeks. You should expect to be given a choice from a few questions and you should write a sensible, referenced, essay in response to one of them. Your paper should be neatly written, well-organized, referenced, and engage deeply with both the literature and practice.</li>
    </ul>

    <h4>Mini-projects</h4>

    <ul>
      <li>Due dates: Weeks 4 and 8.</li>
      <li>Weight: Each is worth 20 per cent; the worst one is dropped.</li>
      <li>Task: You should demo a product that you have built (2-3 min) and then present a sensible plan for how you could monetize (2 min). You will then engage in Q&A (10-15 min). The ideas for these mini-projects will be provided.</li>
    </ul>

    <h4>Main project</h4>

    <ul>
      <li>Due date: First day of exam block.</li>
      <li>Weight: 40 per cent.</li>
      <li>Task: As will the mini-projects, except that the idea should be your own.</li>
    </ul>

    <h3>Related</h3>
    <ul>
      <li>https://mit-mi.github.io/how2ai-course/spring2025/</li>
      <li></li>
    </ul>

  </section>
</div>
</body>
</html>